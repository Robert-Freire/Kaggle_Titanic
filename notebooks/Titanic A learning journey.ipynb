{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titanic. A learning journey\n",
    "\n",
    "## Introduction\n",
    "This is a mix that I'm learning from different Titanic noteboks. The main inspiratiors are\n",
    "\n",
    "* [Titanic Survival Predictions (Beginner)](https://www.kaggle.com/nadintamer/titanic-survival-predictions-beginner)\n",
    "* [A Data Science Framework: To Achieve 99% Accuracy](https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy)\n",
    "\n",
    "Contents: Import Necessary Libraries Read In and Explore the Data Data Analysis Data Visualization Cleaning Data Choosing the Best Model Creating Submission File Any and all feedback is welcome!\n",
    "\n",
    "## Table of Contents\n",
    "1. [Define the problem](#ch1)\n",
    "1. [Read the data](#ch2)\n",
    "1. [Discover correlations](#ch3)\n",
    "1. [Guess the missing ages](#ch4)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id=\"ch1\"></a>\n",
    "\n",
    "## 1. Define the problem\n",
    "\n",
    "First thing first. What we are trying to do? \n",
    "\n",
    "Titanic. Belfast 1912. The largest ship ever made was struck by an iceberg and sank in his maiden vollage. Safety deficencies caused that more tan 1,500 of the 2,224. Our task is guess if a passenger will survive or not.\n",
    "\n",
    "Well, let's do it \n",
    "\n",
    "<a id=\"ch2\"></a>\n",
    "\n",
    "\n",
    "## 2. Read the data\n",
    "\n",
    "To start we begin reading and exploring the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data analysis libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import train and test CSV files\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "#take a look at the training data\n",
    "print (train.info())\n",
    "train.describe(include=\"all\")\n",
    "\n",
    "# A small utility to see correlation\n",
    "def ShowCorrelation (property, ax = None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1,figsize=(8,6))\n",
    "\n",
    "    sns.pointplot(x=property, y=SURVIVED, data=train, ax = ax )\n",
    "    print (train[[SURVIVED,property]].groupby(property, as_index=False).agg(['count', 'mean']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick lock to the content\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where there are missing values\n",
    "missingTrain = pd.isnull(train).sum()\n",
    "print (missingTrain[missingTrain>0])\n",
    "missingTest = pd.isnull(test).sum()\n",
    "print (missingTest[missingTrain>0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data recap\n",
    "\n",
    "   Well, after doig a first look to the data. What we can say?\n",
    "\n",
    "   * PassengerId: Id of the passenger, doesn't seems relevant\n",
    "   * Survived: What we want to know (0/1)\n",
    "   * Pclass: A proxy for SES(Socio-Economic status) (1/2/3). \n",
    "   * Name: String with the name. Could we extract some information from here? \n",
    "   * Sex: Descriptior of the sex (male/female)\n",
    "   * Age: Age of the person. It seemsimportat but we have a lot of nulls. We have to found an strategy assign this number\n",
    "   * SibSp: Number of siblings spouses abroad  (0- 8)\n",
    "   * Parch: Number of parent children abroad (0 - 9)\n",
    "   * Ticket: String Id of the ticket\n",
    "   * Fare: Passenger Fare\n",
    "   * Cabin: Cabin used. A lot of nulls\n",
    "   * Embark: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton). There are 2 for 891 without this data. We can assign to the more used\n",
    "  \n",
    "      \n",
    "\n",
    "   First thoughts\n",
    "\n",
    "   * PClass is a integer but could make sense to tranform to a category?\n",
    "   * Name + SibSp + Parch + ticket + cabin. Could be used to identify families? \n",
    "\n",
    "<a id=\"ch3\"></a>\n",
    "## Discover correlations\n",
    "  \n",
    "    First we will try to discover correlations between values and result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURVIVED = \"Survived\"\n",
    "for x in train:\n",
    "    if ((x != SURVIVED) and (train[x].unique().size <=10)):\n",
    "        print (x, \": \", train[x].unique().size)\n",
    "        print (train[[SURVIVED,x]].groupby(x, as_index=False).agg(['count', 'mean']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw a bar plot of survival by sex\n",
    "SEX = \"Sex\";\n",
    "PCLASS = \"Pclass\";\n",
    "EMBARKED = \"Embarked\";\n",
    "SIBSP = \"SibSp\";\n",
    "PARCH = \"Parch\";\n",
    "AGE = \"Age\";\n",
    "fig, saxis = plt.subplots(2, 3,figsize=(16,12))\n",
    "sns.barplot(x=SEX, y=SURVIVED, data=train, ax =saxis[0,0])\n",
    "sns.barplot(x=PCLASS, y=SURVIVED, data=train, ax =saxis[0,1])\n",
    "sns.barplot(x=EMBARKED, y=SURVIVED, data=train,order=['S','C','Q'], ax=saxis[0,2])\n",
    "sns.pointplot(x=SIBSP , y=SURVIVED,  data=train, ax = saxis[1,0])\n",
    "sns.pointplot(x=PARCH , y=SURVIVED,  data=train, ax = saxis[1,1])\n",
    "sns.pointplot(x=AGE , y=SURVIVED,  data=train, ax = saxis[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Well, this is interesting for what it says and for what is missig.\n",
    "Class, sex are very important\n",
    "also it seems that there are a correlation between size of the family (Parch, sibSp) and probabilities of supervivence\n",
    "what is missing is the age, as is provided is not very useful. We will arrange the age as it is in the notebook of LD Freeman, and by the way, it seems that he is also doing a interesting thing as is extract the Title form the name. Ok, let's do it \n",
    "\n",
    "<a id=\"ch4\"></a>\n",
    "##Guess the missing ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we have for title is this\n",
    "train['Title'] = train['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "test['Title'] = train['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "\n",
    "# as the titles seems related with the age, we can use to fill the age that is missing\n",
    "\n",
    "AverageAgeByTitle = train[['Title',AGE]].groupby('Title', as_index=False).mean();\n",
    "AverageAgeByTitle.reset_index(inplace=True);\n",
    "print(AverageAgeByTitle);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guess the age \n",
    "PASSENGER_ID= \"PassengerId\"\n",
    "TITLE = \"Title\"\n",
    "df = train[[PASSENGER_ID, TITLE, AGE]].merge(AverageAgeByTitle, on=TITLE, how='left');\n",
    "train[AGE][train[AGE].isnull()]=df['Age_y']\n",
    "\n",
    "df = test[[PASSENGER_ID, TITLE, AGE]].merge(AverageAgeByTitle, on=TITLE, how='left');\n",
    "test[AGE][test[AGE].isnull()]=df['Age_y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Survived: What we want to know (0/1)\n",
    "#Pclass: A proxy for SES(Socio-Economic status) (1/2/3).\n",
    "#Name: String with the name. Could we extract some information from here?\n",
    "#Sex: Descriptior of the sex (male/female)\n",
    "#Age: Age of the person\n",
    "#SibSp: Number of siblings spouses abroad (0- 8)\n",
    "#Parch: Number of parent children abroad (0 - 9)\n",
    "#Ticket: String Id of the ticket\n",
    "#Fare: Passenger Fare\n",
    "#Cabin: Cabin used. A lot of nulls\n",
    "#Embark: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "NAME = \"Name\"\n",
    "#train.drop([PASSENGER_ID, NAME]), axis=1, inplace=True);\n",
    "#test.drop([PASSENGER_ID, NAME], axis=1, inplace=True);\n",
    "\n",
    "# Fill NA\n",
    "train[EMBARKED].fillna(train[EMBARKED].mode()[0], inplace = True)\n",
    "test[EMBARKED].fillna(test[EMBARKED].mode()[0], inplace = True)\n",
    "\n",
    "# One Hot Encoder\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "categories = [SEX, EMBARKED]\n",
    "\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train[categories]))\n",
    "OH_cols_test = pd.DataFrame(OH_encoder.transform(test[categories]))\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "train.drop(categories, axis=1)\n",
    "test.drop(categories, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "train = pd.concat([train, OH_cols_train], axis=1)\n",
    "test = pd.concat([test, OH_cols_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AGEBIN = 'AgeBin'\n",
    "#Age\n",
    "train[AGEBIN] = pd.cut(train[AGE].astype(int), 5)\n",
    "test[AGEBIN] = pd.cut(test[AGE].astype(int), 5)\n",
    "ShowCorrelation (AGEBIN);\n",
    "\n",
    "FAMILY_SIZE = 'FamilySize'\n",
    "train[FAMILY_SIZE] = train[SIBSP] + train[PARCH]\n",
    "test[FAMILY_SIZE] = train[SIBSP] + train[PARCH]\n",
    "\n",
    "#OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "#label = LabelEncoder()\n",
    "#train[SEX_CODE] = label.fit_transform(train[SEX])\n",
    "\n",
    "#for dataset in data_cleaner:    \n",
    "#    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n",
    "\n",
    "#for dataset in data_cleaner:    \n",
    "#    #Discrete variables\n",
    "#    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n",
    "#\n",
    "#    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n",
    "#    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n",
    "\n",
    "    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n",
    "#    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "\n",
    "\n",
    "    #Continuous variable bins; qcut vs cut: https://stackoverflow.com/questions/30211923/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n",
    "    #Fare Bins/Buckets using qcut or frequency bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html\n",
    "#    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n",
    "\n",
    "    #Age Bins/Buckets using cut or value bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n",
    "#    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n",
    "\n",
    "\n",
    "    \n",
    "#cleanup rare title names\n",
    "#print(data1['Title'].value_counts())\n",
    "#stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\n",
    "#title_names = (data1['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n",
    "\n",
    "#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https://community.modeanalytics.com/python/tutorial/pandas-groupby-and-python-lambda-functions/\n",
    "#data1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n",
    "#print(data1['Title'].value_counts())\n",
    "#print(\"-\"*10)\n",
    "\n",
    "\n",
    "#preview data again\n",
    "#data1.info()\n",
    "#data_val.info()\n",
    "#data1.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, saxis = plt.subplots(1, 3,figsize=(16,12))\n",
    "ShowCorrelation (SIBSP, saxis[0]);\n",
    "ShowCorrelation (PARCH, saxis[1]);\n",
    "ShowCorrelation (FAMILY_SIZE, saxis[2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the transformer steps and bundle it\n",
    "#1.a Guessing the age from the Title extracted for the name\n",
    "#1.b Assigning the nulls in the embarked port to the mode\n",
    "#1.c Group the age in five groups\n",
    "#1.d. Hot encode sex and port of embark\n",
    "\n",
    "# Pending --- \n",
    "# Create familiSize joining SibSp and arch\n",
    "\n",
    "# To analize\n",
    "# Is good to create an isalone field?S\n",
    "# Makes sense to create a faresBin?\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "#numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "#categorical_transformer = Pipeline(steps=[\n",
    "#    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "#])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "#preprocessor = ColumnTransformer(\n",
    "#    transformers=[\n",
    "#        ('num', numerical_transformer, numerical_cols),\n",
    "#        ('cat', categorical_transformer, categorical_cols)\n",
    "#    ])\n",
    "\n",
    "\n",
    "# 2. Define the model\n",
    "\n",
    "#    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# 3. Setup model and transformers in the Pipeline\n",
    "\n",
    "#from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "#my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                              ('model', model)\n",
    "#                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "#my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "#preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "#score = mean_absolute_error(y_valid, preds)\n",
    "#print('MAE:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('Continuum': virtualenv)",
   "language": "python",
   "name": "python37464bitcontinuumvirtualenv4e9da0f830564f2c806bff7f05bfbd51"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}